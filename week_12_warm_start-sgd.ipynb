{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Warm_start\n",
    "https://frhyme.github.io/python-libs/sklearn_warm_start/\n",
    "- sklearn에서 model의 warm_start의 default는 False임. 이는 .fit을 실행할 때, 이전에 업데이트된 weight(coefficient)를 초기화하고 다시 fitting한다는 것을 의미한다.\n",
    "- 만약 warm_start를 True로 설정하고 진행한다면, .fit을 실행할 때 이전의 weight를 기억하고 그대로 이어서 업데이트되는 것을 말한다.\n",
    "\n",
    "- 일반적인 머신러닝 모델 구축은 그냥 fit - predict가 다죠. 네, 데이터를 전달하고 학습시키고, 예측합니다. 그리고 sklearn의 대부분의 모델들의 메소드도 이렇게 정리되어 있구요.\n",
    "- 다만, 저처럼 컴퓨터의 메모리가 형편없다거나, 하는 경우에는 데이터를 나누어서 학습하는 것이 필요합니다. 한번에 다 넘기면 컴퓨터가 뭔가 느려터지게 되거든요. 따라서, 필요에 따라서 데이터를 나누어 학습하는 것이 필요하다는 이야기죠.네, 텐서플로우를 공부해보신 분은 빠르게 이해하실 텐데, mini-batch로 학습하는 것을 말합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn - fitting with warmstart=FalsePermalink\n",
    "네 일단은 기본적인 sklearn 모델에 대해서 적용해보겠습니다. 보통 다음과 같은 형태로 코드를 작성해서 사용하죠.\n",
    "아래 코드에서는 SGDRegressor이라고하는, Stochastic Gradient Decent를 사용해서 linear model에 대해 coefficient를 업데이트하는 모델을 사용하였고, X, Y를 임의로 만들어주었습니다.\n",
    "그리고, max_iter는 10으로 매우 작게 세팅했죠. 이 아이는 epoch과 동일한데, weight(coefficient)를 업데이트하는 작업을 10번만 하겠다는 말이 되죠. 물론, 이 값만으로는 절대로 converge하지 않습니다만, 우리는 그냥 weight가 어떻게 달라지는지만 볼 꺼니까요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# X, Y\n",
    "X = np.linspace(-1, 1, num=50).reshape(-1, 1)\n",
    "Y = (X*3.3 + 5.6).reshape(50,)\n",
    "\n",
    "# Define Model\n",
    "sgd_reg_model = SGDRegressor(\n",
    "    random_state=0,\n",
    "    verbose=0,\n",
    "    max_iter=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고, 위에서 정의한 모델에 대해서 fitting을 해줍니다.\n",
    "아래 코드에서 보시면, fitting을 두번 해준 것을 알수 있죠. 네 이렇게 하면 한번에 iteration 10번씩 총 20번 해주는 것이 아닐까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== First  fitting\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDRegressor(max_iter=10, random_state=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Coefficient: [1.26229792]\n",
      "== Second fitting\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDRegressor(max_iter=10, random_state=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Coefficient: [1.26229792]\n"
     ]
    }
   ],
   "source": [
    "print(f\"== First  fitting\")\n",
    "sgd_reg_model.fit(X, Y)\n",
    "print(f\"---- Coefficient: {sgd_reg_model.coef_}\")\n",
    "\n",
    "print(f\"== Second fitting\")\n",
    "sgd_reg_model.fit(X, Y)\n",
    "print(f\"---- Coefficient: {sgd_reg_model.coef_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "안타깝게도 그렇지 않습니다. 아래 결과를 보시면, 첫번째 fitting과 두번째 fitting 때의 coefficient가 같은 것을 알 수 있죠. 이는 SGDRegressor를 선언해줄 때, 이 아이의 parameter인 warm_start가 False로 되어 있기 때문이죠. warm_start를 한국말로 번역한다면 “따뜻한 시작”정도가 되겠죠. 즉, 이 값이 True이면, 이전에 학습했던 것을 기억한다는 이야기이고, False이면 이 전에 학습했던 것을 기억하지 않는다는 말이 됩니다.\n",
    "따라서, 아래 코드와 같이 warm_start를 True로 바꾸고 진행하면, 첫번째 fitting할 때와 두번째 fitting할 때의 coefficient가 달라지는 것을 알 수 있죠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== First  fitting\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDRegressor(max_iter=10, random_state=0, warm_start=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Coefficient: [1.26229792]\n",
      "== Second fitting\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDRegressor(max_iter=10, random_state=0, warm_start=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Coefficient: [2.04389939]\n"
     ]
    }
   ],
   "source": [
    "sgd_reg_model = SGDRegressor(\n",
    "    random_state=0,\n",
    "    verbose=0,\n",
    "    max_iter=10,\n",
    "    warm_start=True\n",
    ")\n",
    "print(f\"== First  fitting\")\n",
    "sgd_reg_model.fit(X, Y)\n",
    "print(f\"---- Coefficient: {sgd_reg_model.coef_}\")\n",
    "\n",
    "print(f\"== Second fitting\")\n",
    "sgd_reg_model.fit(X, Y)\n",
    "print(f\"---- Coefficient: {sgd_reg_model.coef_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.0001,\n",
       " 'average': False,\n",
       " 'early_stopping': False,\n",
       " 'epsilon': 0.1,\n",
       " 'eta0': 0.01,\n",
       " 'fit_intercept': True,\n",
       " 'l1_ratio': 0.15,\n",
       " 'learning_rate': 'invscaling',\n",
       " 'loss': 'squared_loss',\n",
       " 'max_iter': 1000,\n",
       " 'n_iter_no_change': 5,\n",
       " 'penalty': 'l2',\n",
       " 'power_t': 0.25,\n",
       " 'random_state': None,\n",
       " 'shuffle': True,\n",
       " 'tol': 0.001,\n",
       " 'validation_fraction': 0.1,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SGDRegressor().get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the difference between partial fit and warm start?\n",
    "I don't know about the Passive Aggressor, but at least when using the SGDRegressor, partial_fit will only fit for 1 epoch, whereas fit will fit for multiple epochs (until the loss converges or max_iter is reached). Therefore, when fitting new data to your model, partial_fit will only correct the model one step towards the new data, but with fit and warm_start it will act as if you would combine your old data and your new data together and fit the model once until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/38052342/what-is-the-difference-between-partial-fit-and-warm-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.24, NNZs: 1, Bias: 0.791393, T: 50, Avg. loss: 1.611747\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.44, NNZs: 1, Bias: 1.274291, T: 100, Avg. loss: 0.701952\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.61, NNZs: 1, Bias: 1.554248, T: 150, Avg. loss: 0.337266\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.75, NNZs: 1, Bias: 1.731325, T: 200, Avg. loss: 0.180598\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.87, NNZs: 1, Bias: 1.837031, T: 250, Avg. loss: 0.105629\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.97, NNZs: 1, Bias: 1.903617, T: 300, Avg. loss: 0.066613\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.06, NNZs: 1, Bias: 1.942107, T: 350, Avg. loss: 0.044076\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.13, NNZs: 1, Bias: 1.964321, T: 400, Avg. loss: 0.030090\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.19, NNZs: 1, Bias: 1.976448, T: 450, Avg. loss: 0.020858\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.24, NNZs: 1, Bias: 1.984309, T: 500, Avg. loss: 0.014577\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.28, NNZs: 1, Bias: 1.990926, T: 550, Avg. loss: 0.010233\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.32, NNZs: 1, Bias: 1.995158, T: 600, Avg. loss: 0.007187\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1.35, NNZs: 1, Bias: 1.997394, T: 650, Avg. loss: 0.005062\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1.37, NNZs: 1, Bias: 1.997556, T: 700, Avg. loss: 0.003567\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1.39, NNZs: 1, Bias: 1.997737, T: 750, Avg. loss: 0.002516\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 1.41, NNZs: 1, Bias: 1.998635, T: 800, Avg. loss: 0.001779\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1.42, NNZs: 1, Bias: 1.999179, T: 850, Avg. loss: 0.001255\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 1.44, NNZs: 1, Bias: 1.999862, T: 900, Avg. loss: 0.000886\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 1.45, NNZs: 1, Bias: 1.999818, T: 950, Avg. loss: 0.000626\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 1.45, NNZs: 1, Bias: 2.000020, T: 1000, Avg. loss: 0.000442\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 1.46, NNZs: 1, Bias: 1.999998, T: 1050, Avg. loss: 0.000353\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000002, T: 1100, Avg. loss: 0.000330\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000005, T: 1150, Avg. loss: 0.000308\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000030, T: 1200, Avg. loss: 0.000287\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000031, T: 1250, Avg. loss: 0.000268\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000030, T: 1300, Avg. loss: 0.000257\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000029, T: 1350, Avg. loss: 0.000253\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000029, T: 1400, Avg. loss: 0.000250\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000028, T: 1450, Avg. loss: 0.000246\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000028, T: 1500, Avg. loss: 0.000243\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000028, T: 1550, Avg. loss: 0.000241\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000028, T: 1600, Avg. loss: 0.000240\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000027, T: 1650, Avg. loss: 0.000240\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000027, T: 1700, Avg. loss: 0.000239\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000027, T: 1750, Avg. loss: 0.000238\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000027, T: 1800, Avg. loss: 0.000238\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000027, T: 1850, Avg. loss: 0.000238\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000027, T: 1900, Avg. loss: 0.000238\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000027, T: 1950, Avg. loss: 0.000237\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000027, T: 2000, Avg. loss: 0.000237\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000027, T: 2050, Avg. loss: 0.000237\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000027, T: 2100, Avg. loss: 0.000237\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000027, T: 2150, Avg. loss: 0.000237\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000027, T: 2200, Avg. loss: 0.000237\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000027, T: 2250, Avg. loss: 0.000237\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000027, T: 2300, Avg. loss: 0.000237\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000027, T: 2350, Avg. loss: 0.000237\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000027, T: 2400, Avg. loss: 0.000237\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000027, T: 2450, Avg. loss: 0.000237\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000027, T: 2500, Avg. loss: 0.000237\n",
      "Total training time: 0.00 seconds.\n",
      "Convergence after 50 epochs took 0.00 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDRegressor(learning_rate='adaptive', max_iter=2000, random_state=0, verbose=1,\n",
       "             warm_start=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "X = np.linspace(-1, 1, num=50).reshape(-1, 1)\n",
    "Y = (X * 1.5 + 2).reshape(50,)\n",
    "\n",
    "modelFit = SGDRegressor(learning_rate=\"adaptive\", eta0=0.01, random_state=0, verbose=1,\n",
    "                     shuffle=True, max_iter=2000, tol=1e-3, warm_start=True)\n",
    "\n",
    "modelPartialFit = SGDRegressor(learning_rate=\"adaptive\", eta0=0.01, random_state=0, verbose=1,\n",
    "                     shuffle=True, max_iter=2000, tol=1e-3, warm_start=False)\n",
    "\n",
    "# first fit some data\n",
    "modelFit.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.24, NNZs: 1, Bias: 0.791393, T: 50, Avg. loss: 1.611747\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.44, NNZs: 1, Bias: 1.274291, T: 100, Avg. loss: 0.701952\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.61, NNZs: 1, Bias: 1.554248, T: 150, Avg. loss: 0.337266\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.75, NNZs: 1, Bias: 1.731325, T: 200, Avg. loss: 0.180598\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.87, NNZs: 1, Bias: 1.837031, T: 250, Avg. loss: 0.105629\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.97, NNZs: 1, Bias: 1.903617, T: 300, Avg. loss: 0.066613\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.06, NNZs: 1, Bias: 1.942107, T: 350, Avg. loss: 0.044076\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.13, NNZs: 1, Bias: 1.964321, T: 400, Avg. loss: 0.030090\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.19, NNZs: 1, Bias: 1.976448, T: 450, Avg. loss: 0.020858\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.24, NNZs: 1, Bias: 1.984309, T: 500, Avg. loss: 0.014577\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.28, NNZs: 1, Bias: 1.990926, T: 550, Avg. loss: 0.010233\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.32, NNZs: 1, Bias: 1.995158, T: 600, Avg. loss: 0.007187\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1.35, NNZs: 1, Bias: 1.997394, T: 650, Avg. loss: 0.005062\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1.37, NNZs: 1, Bias: 1.997556, T: 700, Avg. loss: 0.003567\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1.39, NNZs: 1, Bias: 1.997737, T: 750, Avg. loss: 0.002516\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 1.41, NNZs: 1, Bias: 1.998635, T: 800, Avg. loss: 0.001779\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1.42, NNZs: 1, Bias: 1.999179, T: 850, Avg. loss: 0.001255\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 1.44, NNZs: 1, Bias: 1.999862, T: 900, Avg. loss: 0.000886\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 1.45, NNZs: 1, Bias: 1.999818, T: 950, Avg. loss: 0.000626\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 1.45, NNZs: 1, Bias: 2.000020, T: 1000, Avg. loss: 0.000442\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 1.46, NNZs: 1, Bias: 1.999998, T: 1050, Avg. loss: 0.000353\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000002, T: 1100, Avg. loss: 0.000330\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000005, T: 1150, Avg. loss: 0.000308\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000030, T: 1200, Avg. loss: 0.000287\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000031, T: 1250, Avg. loss: 0.000268\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000030, T: 1300, Avg. loss: 0.000257\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000029, T: 1350, Avg. loss: 0.000253\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000029, T: 1400, Avg. loss: 0.000250\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000028, T: 1450, Avg. loss: 0.000246\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000028, T: 1500, Avg. loss: 0.000243\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000028, T: 1550, Avg. loss: 0.000241\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000028, T: 1600, Avg. loss: 0.000240\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000027, T: 1650, Avg. loss: 0.000240\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000027, T: 1700, Avg. loss: 0.000239\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000027, T: 1750, Avg. loss: 0.000238\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000027, T: 1800, Avg. loss: 0.000238\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000027, T: 1850, Avg. loss: 0.000238\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000027, T: 1900, Avg. loss: 0.000238\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000027, T: 1950, Avg. loss: 0.000237\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000027, T: 2000, Avg. loss: 0.000237\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000027, T: 2050, Avg. loss: 0.000237\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000027, T: 2100, Avg. loss: 0.000237\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000027, T: 2150, Avg. loss: 0.000237\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000027, T: 2200, Avg. loss: 0.000237\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000027, T: 2250, Avg. loss: 0.000237\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000027, T: 2300, Avg. loss: 0.000237\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000027, T: 2350, Avg. loss: 0.000237\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000027, T: 2400, Avg. loss: 0.000237\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000027, T: 2450, Avg. loss: 0.000237\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 1.46, NNZs: 1, Bias: 2.000027, T: 2500, Avg. loss: 0.000237\n",
      "Total training time: 0.00 seconds.\n",
      "Convergence after 50 epochs took 0.00 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDRegressor(learning_rate='adaptive', max_iter=2000, random_state=0, verbose=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelPartialFit.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for both: Convergence after 50 epochs, Norm: 1.46, NNZs: 1, Bias: 2.000027, T: 2500, Avg. loss: 0.000237"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.46303288] [1.46303288]\n"
     ]
    }
   ],
   "source": [
    "print(modelFit.coef_, modelPartialFit.coef_) # for both: [1.46303288]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now fit new data (zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "newX = X\n",
    "newY = 0 * Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 1.23, NNZs: 1, Bias: 1.208630, T: 50, Avg. loss: 1.595492\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDRegressor(learning_rate='adaptive', max_iter=2000, random_state=0, verbose=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fits only for 1 epoch, Norm: 1.23, NNZs: 1, Bias: 1.208630, T: 50, Avg. loss: 1.595492:\n",
    "modelPartialFit.partial_fit(newX, newY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 1.23, NNZs: 1, Bias: 1.208630, T: 50, Avg. loss: 1.595492\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.04, NNZs: 1, Bias: 0.725842, T: 100, Avg. loss: 0.690465\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.87, NNZs: 1, Bias: 0.445650, T: 150, Avg. loss: 0.329124\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.73, NNZs: 1, Bias: 0.268633, T: 200, Avg. loss: 0.174809\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.61, NNZs: 1, Bias: 0.162929, T: 250, Avg. loss: 0.101544\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.51, NNZs: 1, Bias: 0.096413, T: 300, Avg. loss: 0.063724\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.43, NNZs: 1, Bias: 0.057920, T: 350, Avg. loss: 0.042030\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.36, NNZs: 1, Bias: 0.035678, T: 400, Avg. loss: 0.028636\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.30, NNZs: 1, Bias: 0.023499, T: 450, Avg. loss: 0.019825\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.25, NNZs: 1, Bias: 0.015620, T: 500, Avg. loss: 0.013842\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 0.21, NNZs: 1, Bias: 0.009042, T: 550, Avg. loss: 0.009708\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 0.18, NNZs: 1, Bias: 0.004840, T: 600, Avg. loss: 0.006813\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 0.15, NNZs: 1, Bias: 0.002614, T: 650, Avg. loss: 0.004794\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 0.13, NNZs: 1, Bias: 0.002424, T: 700, Avg. loss: 0.003375\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 0.11, NNZs: 1, Bias: 0.002227, T: 750, Avg. loss: 0.002378\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 0.09, NNZs: 1, Bias: 0.001344, T: 800, Avg. loss: 0.001679\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 0.07, NNZs: 1, Bias: 0.000808, T: 850, Avg. loss: 0.001183\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 0.06, NNZs: 1, Bias: 0.000143, T: 900, Avg. loss: 0.000833\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 0.05, NNZs: 1, Bias: 0.000181, T: 950, Avg. loss: 0.000587\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 0.05, NNZs: 1, Bias: 0.000158, T: 1000, Avg. loss: 0.000468\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 0.05, NNZs: 1, Bias: 0.000165, T: 1050, Avg. loss: 0.000436\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 0.05, NNZs: 1, Bias: 0.000145, T: 1100, Avg. loss: 0.000407\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 0.05, NNZs: 1, Bias: 0.000128, T: 1150, Avg. loss: 0.000380\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 0.04, NNZs: 1, Bias: 0.000087, T: 1200, Avg. loss: 0.000354\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 0.04, NNZs: 1, Bias: 0.000085, T: 1250, Avg. loss: 0.000339\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 0.04, NNZs: 1, Bias: 0.000083, T: 1300, Avg. loss: 0.000334\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 0.04, NNZs: 1, Bias: 0.000083, T: 1350, Avg. loss: 0.000330\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 0.04, NNZs: 1, Bias: 0.000081, T: 1400, Avg. loss: 0.000325\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 0.04, NNZs: 1, Bias: 0.000079, T: 1450, Avg. loss: 0.000321\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 0.04, NNZs: 1, Bias: 0.000079, T: 1500, Avg. loss: 0.000318\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 0.04, NNZs: 1, Bias: 0.000079, T: 1550, Avg. loss: 0.000317\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 0.04, NNZs: 1, Bias: 0.000078, T: 1600, Avg. loss: 0.000316\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 0.04, NNZs: 1, Bias: 0.000078, T: 1650, Avg. loss: 0.000315\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 0.04, NNZs: 1, Bias: 0.000078, T: 1700, Avg. loss: 0.000314\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 0.04, NNZs: 1, Bias: 0.000078, T: 1750, Avg. loss: 0.000314\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 0.04, NNZs: 1, Bias: 0.000078, T: 1800, Avg. loss: 0.000314\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 0.04, NNZs: 1, Bias: 0.000078, T: 1850, Avg. loss: 0.000313\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 0.04, NNZs: 1, Bias: 0.000078, T: 1900, Avg. loss: 0.000313\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 0.04, NNZs: 1, Bias: 0.000077, T: 1950, Avg. loss: 0.000313\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 0.04, NNZs: 1, Bias: 0.000077, T: 2000, Avg. loss: 0.000313\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 0.04, NNZs: 1, Bias: 0.000077, T: 2050, Avg. loss: 0.000313\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 0.04, NNZs: 1, Bias: 0.000077, T: 2100, Avg. loss: 0.000313\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 0.04, NNZs: 1, Bias: 0.000077, T: 2150, Avg. loss: 0.000313\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 0.04, NNZs: 1, Bias: 0.000077, T: 2200, Avg. loss: 0.000313\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 0.04, NNZs: 1, Bias: 0.000077, T: 2250, Avg. loss: 0.000313\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 0.04, NNZs: 1, Bias: 0.000077, T: 2300, Avg. loss: 0.000313\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 0.04, NNZs: 1, Bias: 0.000077, T: 2350, Avg. loss: 0.000313\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 0.04, NNZs: 1, Bias: 0.000077, T: 2400, Avg. loss: 0.000313\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 0.04, NNZs: 1, Bias: 0.000077, T: 2450, Avg. loss: 0.000313\n",
      "Total training time: 0.00 seconds.\n",
      "Convergence after 49 epochs took 0.00 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDRegressor(learning_rate='adaptive', max_iter=2000, random_state=0, verbose=1,\n",
       "             warm_start=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convergence after 49 epochs, Norm: 0.04, NNZs: 1, Bias: 0.000077, T: 2450, Avg. loss: 0.000313:\n",
    "modelFit.fit(newX, newY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04245779] [1.22919864]\n"
     ]
    }
   ],
   "source": [
    "print(modelFit.coef_, modelPartialFit.coef_) # [0.04245779] vs. [1.22919864]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08499296] [3.66702685]\n"
     ]
    }
   ],
   "source": [
    "newX = np.reshape([2], (-1, 1))\n",
    "print(modelFit.predict(newX), modelPartialFit.predict(newX)) # [0.08499296] vs. [3.66702685]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If warm_start = False, each subsequent call to .fit() (after an initial call to .fit() or partial_fit()) will reset the model's trainable parameters for the initialisation. If warm_start = True, each subsequent call to .fit() (after an initial call to .fit() or partial_fit()) will retain the values of the model's trainable parameters from the previous run, and use those initially. Regardless of the value of warm_start, each call to partial_fit() will retain the previous run's model parameters and use those initially.\n",
    "\n",
    "Example using MLPRegressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(hidden_layer_sizes=(), max_iter=1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "([array([[0.17009494]])], [array([0.74643783])])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.neural_network\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "x = np.linspace(-1, 1, num=50).reshape(-1, 1)\n",
    "y = (x * 1.5 + 2).reshape(50,)\n",
    "cold_model = sklearn.neural_network.MLPRegressor(hidden_layer_sizes=(), warm_start=False, max_iter=1)\n",
    "warm_model = sklearn.neural_network.MLPRegressor(hidden_layer_sizes=(), warm_start=True, max_iter=1)\n",
    "\n",
    "cold_model.fit(x,y)\n",
    "cold_model.coefs_, cold_model.intercepts_\n",
    "#[array([[0.17009494]])] [array([0.74643783])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(hidden_layer_sizes=(), max_iter=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "([array([[-0.60819342]])], [array([-1.21256186])])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cold_model.fit(x,y)\n",
    "cold_model.coefs_, cold_model.intercepts_\n",
    "#[array([[-0.60819342]])] [array([-1.21256186])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### after second run of .fit(), values are completely different because they were re-initialised before doing the second run for the cold model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(hidden_layer_sizes=(), max_iter=1, warm_start=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "([array([[-1.39815616]])], [array([1.651504])])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(hidden_layer_sizes=(), max_iter=1, warm_start=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "([array([[-1.39715616]])], [array([1.652504])])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warm_model.fit(x,y)\n",
    "warm_model.coefs_, warm_model.intercepts_\n",
    "#[array([[-1.39815616]])] [array([1.651504])]\n",
    "\n",
    "warm_model.fit(x,y)\n",
    "warm_model.coefs_, warm_model.intercepts_\n",
    "#[array([[-1.39715616]])] [array([1.652504])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this time with the warm model, params change relatively little, as params were not re-initialised during second call to .fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(hidden_layer_sizes=(), max_iter=1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "([array([[-0.60719343]])], [array([-1.21156187])])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(hidden_layer_sizes=(), max_iter=1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "([array([[-0.60619347]])], [array([-1.21056189])])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cold_model.partial_fit(x,y)\n",
    "cold_model.coefs_, cold_model.intercepts_\n",
    "#[array([[-0.60719343]])] [array([-1.21156187])]\n",
    "cold_model.partial_fit(x,y)\n",
    "cold_model.coefs_, cold_model.intercepts_\n",
    "#[array([[-0.60619347]])] [array([-1.21056189])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with partial_fit(), params barely change even for cold model, as no re-initialisation occurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color ='blue'> \n",
    "\n",
    "### About difference. Warm start it just an attribute of class. Partial fit it is method of this class. It's basically different things. About same functionalities. \n",
    "    \n",
    "- Yes, partial fit will use self.coef_ because it still needed to get some values to update on training period. And for empty coef_init we just put zero values to self.coef_ and go to the next step of training.\n",
    "\n",
    "- Description.\n",
    "\n",
    "For first start: Whatever how (with or without warm start). We will train on zero coefficients but in result we will save average of our coefficients.\n",
    "\n",
    "- N+1 start:\n",
    "\n",
    "With warm start. We will check via method _allocate_parameter_mem our previous coefficients and take it to train. In result save our average coefficients. Without warm start. We will put zero coefficients (as first start) and go to training step. In result we will still write average coefficients to memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. k-s test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#generate a gaussian r.v X\n",
    "#generate an array of size 1000 - norm stands for normal so this will be gaussian distributed, so X will be X ~N(0,1)\n",
    "x = stats.norm.rvs(size=1000);\n",
    "sns.set_style('whitegrid')\n",
    "sns.kdeplot(np.array(x), bw=0.5)\n",
    "#this should plot a gaussian distribution curve\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare x with normal distribution - this is a positive case, statistic refers to the D_value. Since the pvalue is high we say x follows normal distribution, The pvalue should be 0, which means y does not follow normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KstestResult(statistic=0.027327362827401336, pvalue=0.43635082958022864)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.kstest(x, 'norm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lets look at a negative case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = np.random.uniform(0,1,10000);\n",
    "sns.kdeplot(np.array(y), bw=0.1)\n",
    "#the flat line below should indicate it is an uniform distribution\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KstestResult(statistic=0.5000289032239691, pvalue=0.0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.kstest(y, 'norm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pvalue should be 0, which means y does not follow normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
